---
title: ":orange_book: Encode categorical data"
execute: 
  echo: true
  warning: false
---

# Introduction

## Overview

In this notebook, we will present typical ways of dealing with categorical variables by encoding them, namely ordinal encoding and one-hot encoding.

## Learning objectives

* seen two common strategies for encoding categorical features: ordinal encoding and one-hot encoding;
* used a pipeline to use a one-hot encoder before fitting a logistic regression.

```{r}
#| echo: fenced
library(dplyr)
library(skimr)
```

## Identify categorical data

Let’s first load the entire adult dataset containing both numerical and categorical data.

```{r}
#| echo: fenced
df <- openxlsx::read.xlsx("./data/dataset2.xlsx")
```

::: panel-tabset
#### :pencil2: Exercise 1

Examine the structure of the data, including variable names, labels.

::: callout-tip
-   Stata: use the [codebook](https://www.stata.com/manuals/dcodebook.pdf) command
-   R: use the [skim](https://docs.ropensci.org/skimr/reference/skim.html) function from the `skimr` package
:::

```{r}
#| echo: fenced
# Write your code here
```

#### R

```{r}
#| echo: fenced
df %>%
  skimr::skim()
```
:::

## Ordinal encoding

The most intuitive strategy is to encode each category with a different number. The OrdinalEncoder will transform the data in such manner. We will start by encoding a single column to understand how the encoding works. We see that each category in "education" has been replaced by a numeric value. We could check the mapping between the categories and the numerical values by checking the fitted attribute categories_.

Now, we can check the encoding applied on all categorical features.

We see that the categories have been encoded for each feature (column) independently. We also note that the number of features before and after the encoding is the same.

However, be careful when applying this encoding strategy: using this integer representation leads downstream predictive models to assume that the values are ordered (0 < 1 < 2 < 3… for instance).

By default, OrdinalEncoder uses a lexicographical strategy to map string category labels to integers. This strategy is arbitrary and often meaningless. For instance, suppose the dataset has a categorical variable named "size" with categories such as “S”, “M”, “L”, “XL”. We would like the integer representation to respect the meaning of the sizes by mapping them to increasing integers such as 0, 1, 2, 3. However, the lexicographical strategy used by default would map the labels “S”, “M”, “L”, “XL” to 2, 1, 0, 3, by following the alphabetical order.

The OrdinalEncoder class accepts a categories constructor argument to pass categories in the expected ordering explicitly. You can find more information in the scikit-learn documentation if needed.

If a categorical variable does not carry any meaningful order information then this encoding might be misleading to downstream statistical models and you might consider using one-hot encoding instead (see below).

## Encoding nominal categories (without assuming any order)

OneHotEncoder is an alternative encoder that prevents the downstream models to make a false assumption about the ordering of categories. For a given feature, it will create as many new columns as there are possible categories. For a given sample, the value of the column corresponding to the category will be set to 1 while all the columns of the other categories will be set to 0.

We will start by encoding a single feature (e.g. "education") to illustrate how the encoding works.

Sparse matrices are efficient data structures when most of your matrix elements are zero. They won’t be covered in detail in this course. If you want more details about them, you can look at this. (https://scipy-lectures.org/advanced/scipy_sparse/introduction.html#why-sparse-matrices)

We see that encoding a single feature will give a NumPy array full of zeros and ones. We can get a better understanding using the associated feature names resulting from the transformation.

The number of features after the encoding is more than 10 times larger than in the original data because some variables such as occupation and native-country have many possible categories.

## Choosing an encoding strategy

Choosing an encoding strategy will depend on the underlying models and the type of categories (i.e. ordinal vs. nominal).

Note

In general OneHotEncoder is the encoding strategy used when the downstream models are linear models while OrdinalEncoder is often a good strategy with tree-based models.

Using an OrdinalEncoder will output ordinal categories. This means that there is an order in the resulting categories (e.g. 0 < 1 < 2). The impact of violating this ordering assumption is really dependent on the downstream models. Linear models will be impacted by misordered categories while tree-based models will not.

You can still use an OrdinalEncoder with linear models but you need to be sure that:

the original categories (before encoding) have an ordering;

the encoded categories follow the same ordering than the original categories. The next exercise highlights the issue of misusing OrdinalEncoder with a linear model.

One-hot encoding categorical variables with high cardinality can cause computational inefficiency in tree-based models. Because of this, it is not recommended to use OneHotEncoder in such cases even if the original categories do not have a given order. We will show this in the final exercise of this sequence.
Evaluate our predictive pipeline

We can now integrate this encoder inside a machine learning pipeline like we did with numerical data: let’s train a linear classifier on the encoded data and check the generalization performance of this machine learning pipeline using cross-validation.

Before we create the pipeline, we have to linger on the native-country. Let’s recall some statistics regarding this column.

We see that the Holand-Netherlands category is occurring rarely. This will be a problem during cross-validation: if the sample ends up in the test set during splitting then the classifier would not have seen the category during training and will not be able to encode it.

In scikit-learn, there are two solutions to bypass this issue:

    list all the possible categories and provide it to the encoder via the keyword argument categories;

    use the parameter handle_unknown, i.e. if an unknown category is encountered during transform, the resulting one-hot encoded columns for this feature will be all zeros.

Here, we will use the latter solution for simplicity.

Tip

Be aware the OrdinalEncoder exposes as well a parameter handle_unknown. It can be set to use_encoded_value. If that option is chosen, you can define a fixed value to which all unknowns will be set to during transform. For example, OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=42) will set all values encountered during transform to 42 which are not part of the data encountered during the fit call. You are going to use these parameters in the next exercise.

We can now create our machine learning pipeline.

from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LogisticRegression

model = make_pipeline(
    OneHotEncoder(handle_unknown="ignore"), LogisticRegression(max_iter=500)
)

Note

Here, we need to increase the maximum number of iterations to obtain a fully converged LogisticRegression and silence a ConvergenceWarning. Contrary to the numerical features, the one-hot encoded categorical features are all on the same scale (values are 0 or 1), so they would not benefit from scaling. In this case, increasing max_iter is the right thing to do.

Finally, we can check the model’s generalization performance only using the categorical columns.

from sklearn.model_selection import cross_validate
cv_results = cross_validate(model, data_categorical, target)
cv_results

{'fit_time': array([0.7378273 , 0.65593696, 0.66958189, 0.66641235, 0.65688396]),
 'score_time': array([0.02962923, 0.02977753, 0.02971697, 0.03134513, 0.02978778]),
 'test_score': array([0.83222438, 0.83560242, 0.82872645, 0.83312858, 0.83466421])}

scores = cv_results["test_score"]
print(f"The accuracy is: {scores.mean():.3f} ± {scores.std():.3f}")

The accuracy is: 0.833 ± 0.002

As you can see, this representation of the categorical variables is slightly more predictive of the revenue than the numerical variables that we used previously.