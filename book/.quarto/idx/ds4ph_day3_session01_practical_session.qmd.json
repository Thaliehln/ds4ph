{"title":":orange_book: Practical","markdown":{"yaml":{"title":":orange_book: Practical","warning":false},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\n### Overview\n\nThis tutorial is adapted from the excellent [Machine learning in Python with scikit-learn](https://www.fun-mooc.fr/en/courses/machine-learning-python-scikit-learn/)\n\n### Learning objectives\n\n-   explore data\n-   prepare data\n-   fit a **k-nearest neighbors** model on a training dataset\n-   evaluate its generalization performance on the testing data\n\n## Question\n\nWe are interested in predicting the age of the child based on height and weight measured during the consultation.\n\n-   **MEAS_weight_in_kg** and\n-   **MEAS_height_in_cm**.\n\n```{r}\n#| echo: fenced\nlibrary(tidyverse) # includes dplyr and tibble\nlibrary(skimr)\nlibrary(ggplot2)\nlibrary(DataExplorer)\nlibrary(reticulate)\n```\n\n## Load the data\n\nThe dataset is stored in **dataset4.xlsx**.\n\nRead the dataset and store it into a dataframe called **df**.\n\n```{r}\n#| echo: fenced\ndf <- openxlsx::read.xlsx(\"./data/dataset4.xlsx\")\n```\n\n```{r}\n#| echo: fenced\ndf <- df %>%\n  dplyr::mutate(\n    SDC_age_category = dplyr::case_when(\n      SDC_age_in_months < 12 ~ \"<11 months\",\n      SDC_age_in_months >= 12 & SDC_age_in_months < 36 ~ \"12-35 months\",\n      SDC_age_in_months >= 36 & SDC_age_in_months < 60 ~ \"36-59 months\",\n      TRUE ~ \"\"\n    )\n  ) %>%\n  tibble::remove_rownames() %>%\n  tibble::column_to_rownames(var=\"child_ID\") %>%\n  dplyr::mutate(across(c(SDC_sex,\n                         SDC_age_category), factor))\n```\n\n## Data exploration\n\nWe want to do some data exploration to get an initial understanding of the data. Before building a predictive model, it is always a good idea to look at the data:\n\n-   maybe the task you are trying to achieve can be solved without machine learning;\n-   you need to check that the information you need for your task is actually present in the dataset;\n-   inspecting the data is a good way to find peculiarities. These can arise during data collection (for example, malfunctioning sensor or missing values), or from the way the data is processed afterwards (for example capped values).\n\n### Variables / features\n\n::: panel-tabset\n#### :pencil2: Exercise 1\n\nExamine the structure of the data, including variable names, labels.\n\n1.  How many variables are numerical?\n2.  How many features are categorical?\n\n::: callout-tip\n-   Stata: use the [codebook](https://www.stata.com/manuals/dcodebook.pdf) command\n-   R: use the [skim](https://docs.ropensci.org/skimr/reference/skim.html) function from the `skimr` package\n:::\n\nDisplay the variables/features **child_id**, **test** and **test** for the **10 first observations/samples** in the data.\n\n::: callout-tip\n-   Stata hint: use the the combination of `describe` and `list` command to inspect the data\n-   R hint: you need to load the `dplyr` library to use piping\n:::\n\n```{r}\n#| echo: fenced\n# Write your code here\n```\n\n#### R\n\n```{r}\n#| echo: fenced\ndf %>%\n  skimr::skim()\n```\n:::\n\nNumerical variables can be naturally handled by machine learning algorithms that are typically composed of a sequence of arithmetic instructions such as additions and multiplications.\n\n## Missing data\n\n```{r}\n#| echo: fenced\nDataExplorer::plot_missing(df,\n                           geom_label_args = list(size = 2, label.padding = unit(0.2, \"lines\")))\n```\n\n```{r}\n#| echo: fenced\nDataExplorer::plot_histogram(df)\n```\n\n```{r}\n#| echo: fenced\ndf %>%  ggplot2::ggplot(aes(x = MEAS_height_in_cm,\n                            fill = SDC_age_category)) +\n  geom_histogram(binwidth = 2, alpha = 0.5, position = \"identity\") +\n  theme_minimal()\n```\n\n```{r}\n#| echo: fenced\ndf %>%  ggplot2::ggplot(aes(x = MEAS_weight_in_kg,\n                            fill = SDC_age_category)) +\n  geom_histogram(binwidth = 1, alpha = 0.5, position = \"identity\") +\n  theme_minimal()\n```\n\n```{python}\n#| echo: fenced\nimport seaborn\nimport matplotlib.pyplot\npairplot_figure = seaborn.pairplot(r.df, hue = \"SDC_age_category\")\nmatplotlib.pyplot.show()\n```\n\n### Target classes\n\n::: panel-tabset\n### :pencil2: Exercise 1\n\nWhat are the different age categories available in the dataset and how many observations/samples of each types are there?\n\n::: callout-tip\n-   R: use `table`\n-   Python: select the right column and use the `value_counts` method.\n:::\n\n```{r}\n#| echo: fenced\n# Write your code here\n```\n\n### R\n\n```{r}\n#| echo: fenced\ntable(df$SDC_age_category)\n```\n:::\n\n## Variable/feature distribution\n\nLet's look at the distribution of individual features, to get some insights about the data.\n\nWe can start by plotting histograms, note that this only works for features containing numerical values.\n\n::: panel-tabset\n### :pencil2: Exercise 2\n\nPlot histograms for the numerical variables/features\n\n```{r}\n#| echo: fenced\n# Write your code here\n```\n\n### R\n\n```{r}\n#| echo: fenced\n# Write your code here\n```\n\n```{r}\nDataExplorer::plot_histogram(df)\n```\n:::\n\n::: panel-tabset\n### :pencil2: Exercise 3\n\nShow variable/feature distribution for each age category.\n\nLooking at these distributions, how hard do you think it will be to classify the age category only using height and weight?\n\n```{r}\n#| echo: fenced\n# Write your code here\n```\n\nLooking at the previous scatter-plot showing height and weight, the age categories are reasonably well separated.\n\nThere is some small overlap between the age categories, so we can expect a statistical model to perform well on this dataset but not perfectly.\n\n### R\n\n```{r}\n#| echo: fenced\n# Write your code here\n```\n\n```{r}\n#dummy <- caret::dummyVars(\" ~ .\", data=df)\n```\n:::\n\n## Train-test data split\n\nWhen building a machine learning model, it is important to evaluate the trained model on data that was not used to fit it, as generalization is more than memorization (meaning we want a rule that generalizes to new data, without comparing to data we memorized).\n\nCorrect evaluation is easily done by leaving out a subset of the data when training the model and using it afterwards for model evaluation.\n\nThe data used to fit a model is called **training** data.\n\nThe data used to assess a model is called **testing** data.\n\nWe can load more data, which was actually left-out from the original data set.\n\n### Separate the data and the target\n\nIn general, we do not have separate data sets in two distinct files. Most of the time, we have a single file containing all the data that we need to split once loaded in the memory.\n\n```{r}\ndf <- df[!is.na(df$MEAS_height_in_cm), ]\ndf %>% head(5)\n```\n\n```{r}\n#make this example reproducible\nset.seed(1)\n\n#create ID column\ndf$id <- 1:nrow(df)\n\n#use 70% of dataset as training set and 30% as test set \ntrain_df <- df %>%\n  dplyr::sample_frac(0.70)\ntest_df  <- dplyr::anti_join(df,\n                             train_df, by = 'id')\n```\n\n```{r}\ntrain_target <- train_df[\"SDC_age_category\"] \n```\n\n```{r}\ntest_target <- test_df[\"SDC_age_category\"] \n```\n\n```{r}\ntrain_df <- train_df %>%\n  dplyr::select(-SDC_age_category,\n                -SDC_age_in_months)\n```\n\n```{r}\ntest_df <- test_df %>%\n  dplyr::select(-SDC_age_category,\n                -SDC_age_in_months)\n```\n\n[**`sklearn.neighbors`**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors \"sklearn.neighbors\") provides functionality for unsupervised and supervised neighbors-based learning methods. Unsupervised nearest neighbors is the foundation of many other learning methods, notably manifold learning and spectral clustering. Supervised neighbors-based learning comes in two flavors: [classification](https://scikit-learn.org/stable/modules/neighbors.html#classification) for data with discrete labels, and [regression](https://scikit-learn.org/stable/modules/neighbors.html#regression) for data with continuous labels.\n\nThe principle behind nearest neighbor methods is to find a predefined number of training samples closest in distance to the new point, and predict the label from these. The number of samples can be a user-defined constant (k-nearest neighbor learning), or vary based on the local density of points (radius-based neighbor learning). The distance can, in general, be any metric measure: standard Euclidean distance is the most common choice. Neighbors-based methods are known as *non-generalizing* machine learning methods, since they simply \\\"remember\\\" all of its training data (possibly transformed into a fast indexing structure such as a [Ball Tree](https://scikit-learn.org/stable/modules/neighbors.html#ball-tree) or [KD Tree](https://scikit-learn.org/stable/modules/neighbors.html#kd-tree)).\n\nDespite its simplicity, nearest neighbors has been successful in a large number of classification and regression problems, including handwritten digits and satellite image scenes. Being a non-parametric method, it is often successful in classification situations where the decision boundary is very irregular.\n\n```{python}\n#| echo: fenced\nfrom sklearn.neighbors import KNeighborsClassifier\n\nmodel = KNeighborsClassifier()\n_ = model.fit(r.train_df, r.train_target.values.ravel())\n```\n\n```{python}\n#| echo: fenced\ntarget_predicted = model.predict(r.test_df)\n```\n\n```{r}\n#| echo: fenced\ndata.frame(py$target_predicted) %>%\n  head(5)\n```\n\n```{r}\n#| echo: fenced\ntest_target %>%\n  head(5)\n```\n\n```{r}\n#| echo: fenced\n#| df-print: kable\ncombined <- cbind(test_target, py$target_predicted)\ncolnames(combined) <- c(\"target\", \"prediction\") \ncombined <- combined %>% \n  dplyr::mutate(correct = 1 * (target == prediction))\ncombined %>%  head(5)\n```\n\n```{r}\nperf <- mean(combined$correct)\nperf\n```\n\nNumber of correct prediction: `r perf`\n\n```{python}\naccuracy = model.score(r.test_df, r.test_target)\nmodel_name = model.__class__.__name__\n\nprint(f\"The test accuracy using a {model_name} is \"\n      f\"{accuracy:.3f}\")\n```\n\nIt is harder to conclude on never-seen instances than on already seen ones.\n\nThe score of a model will in general depend on the way we make the training / test split. One downside of doing a single split is that it does not give any information about this variability. Another downside, in a setting where the amount of data is small, is that the data available for training and testing will be even smaller after splitting.\n\n## Estimators\n\n![Scikit-learn navigation map: what machine learning estimators for what data problem](https://scikit-learn.org/stable/_static/ml_map.png){alt=\"Scikit-learn map of machine learning estimators\"}\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","from":"markdown+emoji","output-file":"ds4ph_day3_session01_practical_session.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","bibliography":["references.bib"],"csl":"vancouver.csl","theme":"cosmo","title":":orange_book: Practical"},"extensions":{"book":{"multiFile":true}}},"pdf":{"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","from":"markdown+emoji","output-file":"ds4ph_day3_session01_practical_session.pdf"},"language":{},"metadata":{"block-headings":true,"bibliography":["references.bib"],"csl":"vancouver.csl","documentclass":"scrreprt","title":":orange_book: Practical"},"extensions":{"book":{}}}}}